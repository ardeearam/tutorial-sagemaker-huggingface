{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d96c9ff8-77e3-4135-9f6a-9f7dec8323f1",
   "metadata": {},
   "source": [
    "# Step 1: Prepare the Hugging Face Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9560f824-9442-4b55-9ae2-202a2b35b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541fe53d-7cdc-4577-8349-e97d11a3166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_name = \"dslim/bert-large-NER\"\n",
    "\n",
    "sanitized_model_name = model_name.replace(\"/\", \"-\")\n",
    "\n",
    "#We define the model on both Python and Bash to easily switch between them\n",
    "os.environ[\"MODEL_NAME\"] = model_name\n",
    "os.environ[\"SANITIZED_MODEL_NAME\"] = sanitized_model_name\n",
    "\n",
    "print(sanitized_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4122eb-6d90-4dc7-b8b1-9cd7d3aca901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Save the model locally\n",
    "model.save_pretrained(\"model\")\n",
    "tokenizer.save_pretrained(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f905a58-482b-4767-8bba-e4a191a22c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "mkdir -p model/code\n",
    "cp inference.py model/code/\n",
    "tar -czvf $SANITIZED_MODEL_NAME.tar.gz -C model . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b300d01-0ad3-4e93-a4c6-8842bf66a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "#Optional: Double check the tar file to make sure the required folder hierarchy is achieved\n",
    "tar -tzvf $SANITIZED_MODEL_NAME.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d270e6c4-361d-4db8-9fd4-72e1c827fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 cp $SANITIZED_MODEL_NAME.tar.gz s3://sagemaker.demo.klaudsol.com/\n",
    "rm $SANITIZED_MODEL_NAME.tar.gz\n",
    "rm -rf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfae6b97-d53c-468a-b9a4-7fa16a83d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"arn:aws:iam::XXXXXXXXXXXX:role/service-role/AmazonSageMaker-ExecutionRole-XXXXXXXXTXXXXXX\"\n",
    "model_uri = f\"s3://sagemaker.demo.klaudsol.com/{sanitized_model_name}.tar.gz\"\n",
    "print(role)\n",
    "print(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808d571f-c1df-45fc-a30c-21eaea0001fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "hf_model = HuggingFaceModel(\n",
    "    model_data=model_uri,\n",
    "    role=role,\n",
    "    transformers_version=\"4.26\",\n",
    "    pytorch_version=\"1.13\",\n",
    "    py_version=\"py39\"\n",
    ")\n",
    "print(hf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f1dfc1",
   "metadata": {},
   "source": [
    "# Step 2: Deploy with Serverless Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c5535d-e62c-4df4-aa80-455b3055b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "# Serverless configuration with memory size and concurrency settings\n",
    "serverless_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=6144,  \n",
    "    max_concurrency=1  # Max concurrent requests\n",
    ")\n",
    "\n",
    "# Deploying the model to the serverless endpoint\n",
    "predictor = hf_model.deploy(\n",
    "    initial_instance_count=0,  # Required for serverless inference\n",
    "    serverless_inference_config=serverless_config,\n",
    "    endpoint_name=sanitized_model_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e64a5-273b-48dc-90f3-c95a907f83a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#Optional: Check status of the endpoint\n",
    "aws sagemaker describe-endpoint --endpoint-name $SANITIZED_MODEL_NAME | jq \".EndpointStatus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81c7a72-5650-4e43-929a-ca15c19f7ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {\n",
    "   \"inputs\": \"Hello, I am Pedro Penduko. You can call me Pedro. I live in Manila. I am a member of Data Engineering Pilipinas.\"\n",
    "}\n",
    "\n",
    "# request\n",
    "response = predictor.predict(data)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927659f4-bb58-4907-bc05-84c3f1102b48",
   "metadata": {},
   "source": [
    "# Step 3: Create app that accesses the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac7dc9-6102-49a5-b1e3-f6acd73d1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "test_data = {\"inputs\":\"Hello, I am Pedro Penduko. You can call me Pedro. I live in Manila. I am a member of Data Engineering Pilipinas.\"}\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=sanitized_model_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(test_data)\n",
    ")\n",
    "\n",
    "print(\"Response:\", response[\"Body\"].read().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe34edb6",
   "metadata": {},
   "source": [
    "# Optional: Cleanup Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3740f-326f-442b-84c5-8a57692d6ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b081ea34",
   "metadata": {},
   "source": [
    "# Optional: Cleanup files from Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7aa299-bb6a-4676-a171-93406ec97be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -Rf model\n",
    "rm *.tar.gz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
